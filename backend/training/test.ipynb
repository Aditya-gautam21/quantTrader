{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split  # Fixed: added underscore\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed plotting styles\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  # Fixed: proper style name\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')  # Fixed: f-string, print(f\n",
    "\n",
    "# Cell 2: Hyperparameters\n",
    "DATAPATH = 'rawdata2026-01-13normindicatorsBTCUSDT.csv'  # Fixed spacing\n",
    "TARGET_HORIZON = 1  # predict 1 period ahead\n",
    "HIDDEN_SIZES = [128, 64, 32]  # Fixed list syntax\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Cell 3: Load and prepare data\n",
    "data = pd.read_csv(DATAPATH, index_col=0, parse_dates=True)  # Fixed spacing\n",
    "data.head()  # Fixed: data.\n",
    "\n",
    "# Cell 4: Target creation\n",
    "data['TARGET_RET'] = data['RET1'].shift(-TARGET_HORIZON)  # Fixed spacing/underscores\n",
    "data['TARGET_CLASS'] = (data['TARGET_RET'] > 0).astype(int)\n",
    "clean_data = data.dropna(subset=['TARGET_RET']).copy()\n",
    "print(f\"Dataset size after removing NaN targets: {len(clean_data)}\")\n",
    "print(\"\\nTarget distribution:\\n\", clean_data['TARGET_CLASS'].value_counts())\n",
    "print(\"\\nClass balance:\\n\", clean_data['TARGET_CLASS'].value_counts(normalize=True))\n",
    "\n",
    "# Cell 5: Visualize splits and distributions (fixed plotting)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Train/val/test split visualization (fixed timeindex)\n",
    "y_regression = clean_data['TARGET_RET'].values\n",
    "n = len(y_regression)\n",
    "test_start = int(n * (1 - TEST_SIZE))\n",
    "val_start = int(test_start * (1 - VAL_SIZE / (1 - TEST_SIZE)))\n",
    "time_index = np.arange(len(y_regression))\n",
    "axes[0].plot(time_index[:val_start], y_regression[:val_start], label='Train', alpha=0.7)\n",
    "axes[0].plot(time_index[val_start:test_start], y_regression[val_start:test_start], label='Val', alpha=0.7)\n",
    "axes[0].plot(time_index[test_start:], y_regression[test_start:], label='Test', alpha=0.7)\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Target Return')\n",
    "axes[0].set_title('Train/Val/Test Split Time-Series')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distributions (fixed)\n",
    "axes[1].hist(clean_data['TARGET_RET'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Future Return')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Target Returns')\n",
    "axes[1].axvline(0, color='red', linestyle='--', label='Zero return')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 6: Feature sets for ablation (new: added)\n",
    "excluded_cols = ['TARGET_RET', 'TARGET_CLASS']\n",
    "feature_cols = [col for col in clean_data.columns if col not in excluded_cols]\n",
    "\n",
    "feature_sets = {\n",
    "    'all_features': feature_cols,\n",
    "    'momentum_only': ['RSI14', 'MACDHIST', 'RET1', 'RET5', 'RET15'],\n",
    "    'volatility_only': ['ATR', 'RET_STD', 'BB_POSITION', 'BODY_ATR', 'UPPER_WICK_ATR', 'LOWER_WICK_ATR'],\n",
    "    'price_position': ['PRICE_EMA21_DIST', 'VWAP_DIST', 'BB_POSITION'],\n",
    "    'volume_based': ['VOL', 'VWAP_DIST'],\n",
    "    'hybrid_best': ['RSI14', 'MACDHIST', 'ATR', 'VOL', 'VWAP_DIST', 'RET1'],\n",
    "    'minimal': ['RET1', 'ATR', 'VOL']\n",
    "}\n",
    "\n",
    "x_datasets = {}\n",
    "for set_name, feature_list in feature_sets.items():\n",
    "    x_datasets[set_name] = clean_data[feature_list].values\n",
    "    print(f\"{set_name:<20s}: {len(feature_list)} features\")\n",
    "\n",
    "y_regression = clean_data['TARGET_RET'].values\n",
    "y_classification = clean_data['TARGET_CLASS'].values\n",
    "\n",
    "# Cell 7: Time-series split function (new: added)\n",
    "def create_time_series_splits(X_data, y_data, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"Split time series data chronologically.\"\"\"\n",
    "    n = len(X_data)\n",
    "    test_start = int(n * (1 - test_size))\n",
    "    val_start = int(test_start * (1 - val_size / (1 - test_size)))\n",
    "    \n",
    "    x_train = X_data[:val_start]\n",
    "    x_val = X_data[val_start:test_start]\n",
    "    x_test = X_data[test_start:]\n",
    "    y_train = y_data[:val_start]\n",
    "    y_val = y_data[val_start:test_start]\n",
    "    y_test = y_data[test_start:]\n",
    "    \n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "# Create splits for all feature sets\n",
    "splits = {}\n",
    "for dataset_name, X_data in x_datasets.items():\n",
    "    x_tr, x_v, x_te, y_tr, y_v, y_te = create_time_series_splits(\n",
    "        X_data, y_regression, test_size=TEST_SIZE, val_size=VAL_SIZE\n",
    "    )\n",
    "    splits[dataset_name] = {\n",
    "        'x_train': x_tr, 'x_val': x_v, 'x_test': x_te,\n",
    "        'y_train': y_tr, 'y_val': y_v, 'y_test': y_te\n",
    "    }\n",
    "    print(f\"{dataset_name:<15s}: train={len(x_tr)}, val={len(x_v)}, test={len(x_te)}\")\n",
    "\n",
    "# Cell 8: Dataset class (new: fixed TimeSeriesDataset â†’ TradingDataset)\n",
    "class TradingDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Cell 9: Model class (fixed PricePredictionNN, added SimpleNN)\n",
    "class PricePredictionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64, 32], dropout=0.3):\n",
    "        super(PricePredictionNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class SimpleNN(nn.Module):  # Added for ablation\n",
    "    def __init__(self, input_size, hidden_sizes, dropout):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Example model creation (using all_features)\n",
    "input_size = x_datasets['all_features'].shape[1]\n",
    "model = PricePredictionNN(input_size=input_size).to(device)\n",
    "print(\"Model Architecture:\\n\", model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Cell 10: Train/Val functions (fixed)\n",
    "def train_epochs(model, train_loader, optimiser, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device).unsqueeze(1)  # Fixed unsqueeze\n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(x_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def val_epochs(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device).unsqueeze(1)\n",
    "            predictions = model(x_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Cell 11: DataLoaders example (using all_features split)\n",
    "data = splits['all_features']\n",
    "train_dataset = TradingDataset(data['x_train'], data['y_train'])\n",
    "val_dataset = TradingDataset(data['x_val'], data['y_val'])\n",
    "test_dataset = TradingDataset(data['x_test'], data['y_test'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)  # No shuffle for time series\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created. Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Cell 12: Full training loop with early stopping (new: complete)\n",
    "optimiser = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epochs(model, train_loader, optimiser, criterion, device)\n",
    "    val_loss = val_epochs(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Fixed: proper save\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nStopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth', weights_only=True))  # Fixed: weights_only=True\n",
    "print(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "# Cell 13: Plot training history (fixed)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "ax.plot(epochs_range, train_losses, label='Training Loss', linewidth=2)\n",
    "ax.plot(epochs_range, val_losses, label='Validation Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Training history plot saved to training_history.png\")\n",
    "\n",
    "# Cell 14: Test evaluation (new: complete with metrics)\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        predictions = model(x_batch).cpu().numpy().flatten()\n",
    "        all_predictions.extend(predictions)\n",
    "        all_actuals.extend(y_batch.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_actuals = np.array(all_actuals)\n",
    "\n",
    "test_loss = criterion(torch.FloatTensor(all_predictions), torch.FloatTensor(all_actuals)).item()\n",
    "test_rmse = np.sqrt(test_loss)\n",
    "\n",
    "pred_direction = (all_predictions > 0).astype(int)\n",
    "actual_direction = (all_actuals > 0).astype(int)\n",
    "directional_accuracy = (pred_direction == actual_direction).mean()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss (MSE): {test_loss:.6f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"Directional Accuracy: {directional_accuracy:.2%}\")\n",
    "print(f\"Random Baseline: 50.00%\")\n",
    "print(f\"Improvement: {(directional_accuracy - 0.5)*100:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cell 15: Feature ablation (run for all sets - new: complete loop)\n",
    "results = {}\n",
    "for dataset_name in list(splits.keys())[:3]:  # Limit to first 3 for demo\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Training on {dataset_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    data = splits[dataset_name]\n",
    "    train_dataset = TradingDataset(data['x_train'], data['y_train'])\n",
    "    val_dataset = TradingDataset(data['x_val'], data['y_val'])\n",
    "    test_dataset = TradingDataset(data['x_test'], data['y_test'])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    input_size = data['x_train'].shape[1]\n",
    "    model = SimpleNN(input_size, HIDDEN_SIZES, DROPOUT).to(device)  # Use SimpleNN for ablation\n",
    "    optimiser = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Quick training (20 epochs for demo)\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(20):\n",
    "        train_loss = train_epochs(model, train_loader, optimiser, criterion, device)\n",
    "        val_loss = val_epochs(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_acts = []\n",
    "    with torch.no_grad():\n",
    "        for x_b, y_b in test_loader:\n",
    "            x_b = x_b.to(device)\n",
    "            preds = model(x_b).cpu().numpy().flatten()\n",
    "            all_preds.extend(preds)\n",
    "            all_acts.extend(y_b.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_acts = np.array(all_acts)\n",
    "    test_loss = criterion(torch.FloatTensor(all_preds), torch.FloatTensor(all_acts)).item()\n",
    "    pred_dir = (all_preds > 0).astype(int)\n",
    "    act_dir = (all_acts > 0).astype(int)\n",
    "    dir_acc = (pred_dir == act_dir).mean()\n",
    "    \n",
    "    results[dataset_name] = {'test_loss': test_loss, 'directional_accuracy': dir_acc, 'num_features': input_size}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ABLATION STUDY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Feature Set':<20} {'Num Features':<15} {'Dir Accuracy':<15} {'Test Loss':<15}\")\n",
    "print(\"-\"*80)\n",
    "for dataset_name, metrics in results.items():\n",
    "    print(f\"{dataset_name:<20} {metrics['num_features']:<15} {metrics['directional_accuracy']:<15.2%} {metrics['test_loss']:<15.4f}\")\n",
    "\n",
    "best_set = max(results.items(), key=lambda x: x[1]['directional_accuracy'])\n",
    "print(\"-\"*80)\n",
    "print(f\"Best performing set: {best_set[0]} with {best_set[1]['directional_accuracy']:.2%} accuracy\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
